{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import random as r\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV,KFold,RandomizedSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import balanced_accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import tree\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTree(Data,Target,k_fold,Data_test,Target_test,type):\n",
    "    pipeline_tree = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('tree', tree.DecisionTreeClassifier())\n",
    "    ])\n",
    "    cv_inner = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    # define search space\n",
    "    space = dict()\n",
    "    space['tree__min_samples_leaf'] = list(range(50,200,25))\n",
    "    space['tree__max_depth'] = list(range(10,60,10))\n",
    "    space['tree__max_leaf_nodes'] = list(range(10,200,10))\n",
    "    # define search\n",
    "    search = RandomizedSearchCV(pipeline_tree, space, scoring='balanced_accuracy', n_jobs=1, cv=cv_inner, refit=True, n_iter = 100)\n",
    "    # configure the cross-validation procedure\n",
    "    cv_outer = KFold(n_splits=k_fold, shuffle=True, random_state=1)\n",
    "    # execute the nested cross-validation\n",
    "    scores_tree = cross_validate(search, Data, Target, cv=cv_outer, n_jobs=-1,return_estimator=True)\n",
    "    #print(scores_tree['estimator'].best_params_)\n",
    "    if type == \"class\":\n",
    "        proba_tree = cross_val_predict(search, Data, Target, cv=cv_outer, n_jobs=-1,method = \"predict_proba\")\n",
    "        proba_tree = proba_tree[:,1]\n",
    "    else:\n",
    "        proba_tree = cross_val_predict(search, Data, Target, cv=cv_outer, n_jobs=-1)\n",
    "    scores__tree = cross_val_score(search, Data, Target, cv=cv_outer, n_jobs=-1).mean()\n",
    "    parameters_tree = []\n",
    "    for j in range(k_fold):\n",
    "        parameters_tree.append(scores_tree['estimator'][j].best_params_)\n",
    "    search.fit(Data, Target)\n",
    "    preds_tree_test = search.predict(Data_test) \n",
    "    if type == \"class\":\n",
    "        proba_tree_test = search.predict_proba(Data_test)[:,1]\n",
    "        score_final_tree = balanced_accuracy_score(preds_tree_test,Target_test) \n",
    "    else: \n",
    "        proba_tree_test = preds_tree_test\n",
    "        score_final_tree = mean_squared_error(preds_tree_test,Target_test)    \n",
    "    return proba_tree, scores__tree, search.best_params_,proba_tree_test,score_final_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_bruh_Regression(Data,Target,k_fold,Data_test,Target_test,type):\n",
    "    pipeline_LR = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('LR', LogisticRegression(solver = \"liblinear\"))\n",
    "    ])\n",
    "    cv_inner = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    # define search space\n",
    "    space = dict()\n",
    "    space['LR__penalty'] = [\"l1\",\"l2\"]\n",
    "    space['LR__C'] = [0.01,0.1,0.3,0.5,1,5,10,100]\n",
    "    # define search\n",
    "    search = GridSearchCV(pipeline_LR, space, scoring='balanced_accuracy', n_jobs=1, cv=cv_inner, refit=True)\n",
    "    # configure the cross-validation procedure\n",
    "    cv_outer = KFold(n_splits=k_fold, shuffle=True, random_state=1)\n",
    "    # execute the nested cross-validation\n",
    "    scores_LR = cross_validate(search, Data, Target, cv=cv_outer, n_jobs=-1,return_estimator=True)\n",
    "    #print(scores_tree['estimator'].best_params_)\n",
    "    if type == \"class\":\n",
    "        proba_LR = cross_val_predict(search, Data, Target, cv=cv_outer, n_jobs=-1,method = \"predict_proba\")\n",
    "        proba_LR = proba_LR[:,1]\n",
    "    else:\n",
    "        proba_LR = cross_val_predict(search, Data, Target, cv=cv_outer, n_jobs=-1)\n",
    "    scores__LR = cross_val_score(search, Data, Target, cv=cv_outer, n_jobs=-1).mean()\n",
    "    parameters_LR = []\n",
    "    for i in range(k_fold):\n",
    "        parameters_LR.append(scores_LR['estimator'][i].best_params_)\n",
    "    search.fit(Data, Target)\n",
    "    preds_LR_test = search.predict(Data_test)\n",
    "    if type == \"class\":\n",
    "        proba_LR_test = search.predict_proba(Data_test)[:,1]\n",
    "        score_final_LR = balanced_accuracy_score(preds_LR_test,Target_test) \n",
    "    else: \n",
    "        proba_LR_test = preds_LR_test\n",
    "        score_final_LR = mean_squared_error(preds_LR_test,Target_test)\n",
    "    return proba_LR, scores__LR, search.best_params_,proba_LR_test,score_final_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA(Data,Target,k_fold,Data_test,Target_test):\n",
    "    pipeline_LDA = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', LinearDiscriminantAnalysis())\n",
    "    ])\n",
    "    proba_LDA = cross_val_predict(pipeline_LDA, Data, Target, cv=k_fold, n_jobs=-1,method = \"predict_proba\")\n",
    "    proba_LDA = proba_LDA[:,1]\n",
    "    scores__LDA = cross_val_score(pipeline_LDA, Data, Target, cv=k_fold, n_jobs=-1).mean()\n",
    "    pipeline_LDA.fit(Data,Target)\n",
    "    proba_LDA_test = pipeline_LDA.predict_proba(Data_test)[:,1]\n",
    "    preds_LDA_test = pipeline_LDA.predict(Data_test)\n",
    "    score_final_LDA = balanced_accuracy_score(preds_LDA_test,Target_test)\n",
    "    return proba_LDA,scores__LDA, proba_LDA_test,score_final_LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QDA(Data,Target,k_fold,Data_test,Target_test):\n",
    "    pipeline_QDA = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', QuadraticDiscriminantAnalysis())\n",
    "    ])\n",
    "    proba_QDA = cross_val_predict(pipeline_QDA, Data, Target, cv=k_fold, n_jobs=-1,method = \"predict_proba\")\n",
    "    proba_QDA = proba_QDA[:,1] \n",
    "    scores__QDA = cross_val_score(pipeline_QDA, Data, Target, cv=k_fold, n_jobs=-1).mean()\n",
    "    pipeline_QDA.fit(Data,Target)\n",
    "    proba_QDA_test = pipeline_QDA.predict_proba(Data_test)[:,1]\n",
    "    preds_QDA_test = pipeline_QDA.predict(Data_test)\n",
    "    score_final_QDA = balanced_accuracy_score(preds_QDA_test,Target_test)\n",
    "    return proba_QDA, scores__QDA,proba_QDA_test,score_final_QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(Data,Target,k_fold,Data_test,Target_test,type):\n",
    "    pipeline_KNN = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "    ])\n",
    "    cv_inner = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    # define search space\n",
    "    space = dict()\n",
    "    space['knn__n_neighbors'] = list(range(3,20,1))\n",
    "    # define search\n",
    "    search = GridSearchCV(pipeline_KNN, space, scoring='balanced_accuracy', n_jobs=1, cv=cv_inner, refit=True)\n",
    "    # configure the cross-validation procedure\n",
    "    cv_outer = KFold(n_splits=k_fold, shuffle=True, random_state=1)\n",
    "    # execute the nested cross-validation\n",
    "    scores_KNN = cross_validate(search, Data, Target, cv=cv_outer, n_jobs=-1,return_estimator=True)\n",
    "    #print(scores_tree['estimator'].best_params_)\n",
    "    if type == \"class\":\n",
    "        proba_KNN = cross_val_predict(search, Data, Target, cv=cv_outer, n_jobs=-1,method = \"predict_proba\")\n",
    "        proba_KNN = proba_KNN[:,1]\n",
    "    else:\n",
    "        proba_KNN = cross_val_predict(search, Data, Target, cv=cv_outer, n_jobs=-1)\n",
    "    scores__KNN = cross_val_score(search, Data, Target, cv=cv_outer, n_jobs=-1).mean()\n",
    "    parameters_KNN = []\n",
    "    for i in range(k_fold):\n",
    "        parameters_KNN.append(scores_KNN['estimator'][i].best_params_)\n",
    "    search.fit(Data, Target)\n",
    "    preds_KNN_test = search.predict(Data_test)\n",
    "    if type == \"class\":\n",
    "        proba_KNN_test = search.predict_proba(Data_test)[:,1]\n",
    "        score_final_KNN = balanced_accuracy_score(preds_KNN_test,Target_test) \n",
    "    else: \n",
    "        proba_KNN_test = preds_KNN_test\n",
    "        score_final_KNN = mean_squared_error(preds_KNN_test,Target_test)\n",
    "    return proba_KNN,scores__KNN,search.best_params_,proba_KNN_test,score_final_KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stacking_Classifier_F(Data,Target,Models,type,k_fold= 5,again = 1,n_layers = 1,Data_test=None,Target_test=None,Meta_estimator=\"KNeighbors\"):\n",
    "    '''\n",
    "    Jest to metoda uczenia maszynowego służąca do regresji oraz klasyfikacji binarnej (dla klas {-1,1}) typu ensemble, działający poprzez generowanie finalnych predykcji za\n",
    "    pomocą meta estymatora, który uczy się przewidywania na podstawie zarówno danych wejściowych ORAZ predykcji z tych danych wygenerowanych przez metody składowe podane\n",
    "    przez użytkownika za pomocą cross_val_predict.\n",
    "\n",
    "    Parametry funkcji:\n",
    "    Data - zbior treningowy (bez zmiennej celu)\n",
    "    Target - treningowe \"etykiety\"\n",
    "    Data_test - zbiór testowy\n",
    "    Target_test - testowe \"etykiety\"\n",
    "    type - typ problemu; dla klasyfikacji binarnej: \"class\", dla regresji: \"reg\"\n",
    "    Models - lista zawierająca nazwy modeli użytych do stackingu, możliwe modele to: DecisionTreeClassifier,LogisticRegression,LDA,QDA,Knn, gdzie użytkownik podaje nazwy\n",
    "    metod jako lista. Możliwe metody to \"DecisionTree\", \"LogisticRegression\",\"LDA\",\"QDA\",\"KNeighbors\" i użytkownik umieszcza je na liście np. \n",
    "    [\"DecisionTreeClassifier\", \"LogisticRegression\"]. Uwaga! przy regresji nie można używać metod \"LDA\" oraz \"QDA\" \n",
    "    k_fold - liczność podziału w kroswalidacji (zewnętrznej, wewnętrzna kroswalidacja jest 5-krotna)\n",
    "    n_layers - liczba warstw stackingu(domyślnie 1), np. 1,2,3,... - Uwaga! - n_layers >= 3 może być bardzo czasochłonne przy wywołaniu.\n",
    "    again - zmienna mówiąca w ilu pierwszych warstwach stackingu użyć również danych treningowych oryginalnych (domyślnie 1), przyjmująca wartości 0,1,2,...,\n",
    "    czyli again = 0 oznacza użycie danych tylko na samym początku, potem same predykcje, oczywiście again <= n_layers\n",
    "    Meta_estimator - metoda używana do generowania finalnych predykcji przy użyciu wcześniejszych predykcji, możliwe metody to: Knn, Svm oraz średnia arytmetyczna, domyślnie Knn,\n",
    "    wpisujemy je odpowiednio jako \"KNeighbors\", \"SVM\" lub \"mean\". Uwaga! dla regresji nie można stosować metody SVM \n",
    "\n",
    "    Funkcja zwraca listę zawierającą modele składowe oraz model finalny (stacking), ich balanced_accuracy_score lub (MSE dla regresji) na danych testowych \n",
    "    oraz hiperparametry(modeli składowych), gdzie modele składowe to modele oparte na danych BAZOWYCH, Uwaga! Dane są skalowane w funkcji, co zmienia MSE\n",
    "    '''\n",
    "    final_scores = []\n",
    "    final_test_scores = []\n",
    "    # Warstwy:\n",
    "    Data_final_1 = copy.deepcopy(Data)\n",
    "    Data_final_test = copy.deepcopy(Data_test)\n",
    "    p = len(Data.axes[1])         \n",
    "    for i in range(n_layers):   \n",
    "        if \"DecisionTree\" in Models:\n",
    "            proba_tree,scores__tree,parameters_tree,proba_tree_test,score_final_tree = DecisionTree(Data_final_1,Target,k_fold,Data_final_test,Target_test,type)\n",
    "            Data_final_1[f'pred_tree_{i}'] = proba_tree.tolist()\n",
    "            Data_final_test[f'pred_tree_{i}'] = proba_tree_test.tolist()\n",
    "            final_scores.append([\"Tree\",scores__tree,i]) \n",
    "            final_test_scores.append([\"Tree\",score_final_tree,parameters_tree,i])\n",
    "        if \"Logistic_Regression\" in Models:\n",
    "            proba_LR,scores__LR,parameters_LR,proba_LR_test,score_final_LR = Logistic_bruh_Regression(Data_final_1,Target,k_fold,Data_final_test,Target_test,type)\n",
    "            Data_final_1[f'pred_LR_{i}'] = proba_LR.tolist()\n",
    "            Data_final_test[f'pred_LR_{i}'] = proba_LR_test.tolist() \n",
    "            final_scores.append([\"LR\",scores__LR,i])\n",
    "            final_test_scores.append([\"LR\",score_final_LR,parameters_LR,i])\n",
    "        if \"LDA\" in Models:\n",
    "            proba_LDA,scores__LDA,proba_LDA_test,score_final_LDA = LDA(Data_final_1,Target,k_fold,Data_final_test,Target_test,type)\n",
    "            Data_final_1[f'pred_LDA_{i}'] = proba_LDA.tolist() \n",
    "            Data_final_test[f'pred_LDA_{i}'] = proba_LDA_test.tolist()\n",
    "            final_scores.append([\"LDA\",scores__LDA,i]) \n",
    "            final_test_scores.append([\"LDA\",score_final_LDA,\"\",i])           \n",
    "        if \"QDA\" in Models:\n",
    "            proba_QDA,scores__QDA,proba_QDA_test,score_final_QDA = QDA(Data_final_1,Target,k_fold,Data_final_test,Target_test,type)\n",
    "            Data_final_1[f'pred_QDA_{i}'] = proba_QDA.tolist() \n",
    "            Data_final_test[f'pred_QDA_{i}'] = proba_QDA_test.tolist()\n",
    "            final_scores.append([\"QDA\",scores__QDA,i])\n",
    "            final_test_scores.append([\"QDA\",score_final_QDA,\"\",i])\n",
    "        if \"KNeighbors\" in Models:\n",
    "            proba_KNN,scores__KNN,parameters_KNN,proba_KNN_test,score_final_KNN = KNN(Data_final_1,Target,k_fold,Data_final_test,Target_test,type)\n",
    "            Data_final_1[f'pred_KNN_{i}'] = proba_KNN.tolist() \n",
    "            Data_final_test[f'pred_KNN_{i}'] = proba_KNN_test.tolist()\n",
    "            final_scores.append([\"KNN\",scores__KNN,i])\n",
    "            final_test_scores.append([\"KNN\",score_final_KNN,parameters_KNN,i])\n",
    "        if i == again:\n",
    "            Data_final_1 =  Data_final_1.drop(Data_final_1.columns[range(p)],axis = 1)\n",
    "            Data_final_test =  Data_final_test.drop(Data_final_test.columns[range(p)],axis = 1) \n",
    "    #final_scores.sort(key=lambda x: x[1]) \n",
    "    #best_model = [final_scores[-1][0],final_scores[-1][2]]\n",
    "    final_test_scores_1 = []\n",
    "    for i in final_test_scores:\n",
    "        if i[3] == 0:\n",
    "            final_test_scores_1.append(i)\n",
    "    # Meta klasyfikator\n",
    "    if Meta_estimator == \"KNeighbors\":        \n",
    "        pipeline_meta = Pipeline([\n",
    "        ('knn', KNeighborsClassifier())\n",
    "        ])\n",
    "        space = dict()\n",
    "        space['knn__n_neighbors'] = list(range(8,25,1))\n",
    "        search = GridSearchCV(pipeline_meta, space, scoring='balanced_accuracy', n_jobs=1, cv=5)\n",
    "        search.fit(Data_final_1,Target)\n",
    "        preds = search.predict(Data_final_test)\n",
    "    if Meta_estimator == \"SVM\":        \n",
    "        pipeline_meta = Pipeline([\n",
    "        ('svm', SVC())\n",
    "        ])\n",
    "        space = dict()\n",
    "        space['svm__C'] = [0.001,0.005,0.01,0.05,0.1,0.5,0,1,5,10,50,100]\n",
    "        search = GridSearchCV(pipeline_meta, space, scoring='balanced_accuracy', n_jobs=1, cv=5)\n",
    "        search.fit(Data_final_1,Target)\n",
    "        preds = search.predict(Data_final_test) \n",
    "    if Meta_estimator == \"mean\":\n",
    "        Data_final_test =  Data_final_test.drop(Data_final_test.columns[range(p)],axis = 1)\n",
    "        Data_final_test['preds'] = Data_final_test.mean(axis=1)\n",
    "        preds = Data_final_test['preds'].to_numpy()  \n",
    "        preds = np.where(preds >= 0.5,1,-1)     \n",
    "    if type == \"class\":\n",
    "       final_test_scores_1.append([\"Stack\",balanced_accuracy_score(preds,Target_test),1])\n",
    "    else:\n",
    "       final_test_scores_1.append([\"Stack\",mean_squared_error(preds,Target_test),1])        \n",
    "    final_test_scores_1.sort(key=lambda x: x[1],reverse=True)\n",
    "    #acc = f\"Stack accuracy: {balanced_accuracy_score(preds,Target_test)}\"\n",
    "    final_test_scores_1 = [final_test_scores_1[i][:-1] for i in range(len(final_test_scores_1))]\n",
    "    return final_test_scores_1\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "           \n",
    "    \n",
    "            \n",
    "    \n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"C:\\\\Users\\\\User\\\\Desktop\\\\Python_5sem\\\\artifical_train_data.csv\")\n",
    "y = pd.read_csv(\"C:\\\\Users\\\\User\\\\Desktop\\\\Python_5sem\\\\artifical_train_labels.csv\")\n",
    "y = np.array(y)\n",
    "y = y.flatten()\n",
    "X = X.iloc[:,list(range(22,30,1))]\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2,shuffle=True)\n",
    "values, counts = np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['KNN', 0.8774647182464218, {'knn__n_neighbors': 5}],\n",
       " ['Stack', 0.8405253283302063],\n",
       " ['Tree',\n",
       "  0.7433461034378466,\n",
       "  {'tree__min_samples_leaf': 50,\n",
       "   'tree__max_leaf_nodes': 10,\n",
       "   'tree__max_depth': 50}]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stacking_Classifier_F(Data = X_train,Target = y_train,Models =[\"DecisionTree\",\"KNeighbors\"],type = \"class\",again = 1,n_layers = 1,Data_test = X_test, Target_test=y_test,Meta_estimator=\"KNeighbors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8846779237023139,\n",
       " 0.8477509949688369,\n",
       " 0.8298936835522202,\n",
       " 0.8798172277874017,\n",
       " 0.8645903859174001,\n",
       " 0.8447594436421677,\n",
       " 0.8625090627265681,\n",
       " 0.87508793086122,\n",
       " 0.8616690488728402,\n",
       " 0.8717653247826849]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "for i in range(10):\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2,shuffle=True)\n",
    "    l.append(Stacking_Classifier_F(Data = X_train,Target = y_train,Models =[\"KNeighbors\",\"DecisionTree\"],again = 1,n_layers = 2,Data_test = X_test, Target_test=y_test))\n",
    "l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8622521026813654, 0.016235538274587343)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln = np.array(l)\n",
    "ln.mean(),ln.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta classifier = KNN, classifiers = DecisionTree, 8 kolumn\n",
    "# again = 1, n_layers = 1, rózne splity, srednia 0.85, sigma = 0.025\n",
    "# again = 1, n_layers = 3, rózne splity, srednia 0.77, sigma = 0.022\n",
    "# again = 3, n_layers = 3, rózne splity, srednia 0.84, sigma = 0.02\n",
    "\n",
    "# meta classifier = KNN, classifiers = KNeighbors, 8 kolumn\n",
    "# again = 1, n_layers = 1, rózne splity, srednia 0.84, sigma = 0.019\n",
    "# again = 1, n_layers = 2, rózne splity, srednia 0.88, sigma = 0.018\n",
    "# again = 1, n_layers = 3, rózne splity, srednia 0.89, sigma = 0.015\n",
    "# again = 1, n_layers = 4, rózne splity, srednia 0.89, sigma = 0.011\n",
    "# again = 1, n_layers = 5, rózne splity, srednia 0.89, sigma = 0.012\n",
    "\n",
    "# meta classifier = KNN, classifiers = DecisionTree,KNeighbors, 8 kolumn\n",
    "# again = 1, n_layers = 1, rózne splity, srednia 0.84, sigma = 0.015\n",
    "# again = 1, n_layers = 2, rózne splity, srednia 0.86, sigma = 0.016"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
